{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abdulrahman-Aladdin/AI_MDP/blob/main/MDP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI Assignment 3\n",
        "## Markov Desicion Process\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZN4G3U_X5h-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colorama\n",
        "from colorama import Fore"
      ],
      "metadata": {
        "id": "-13GAZ-QfeMU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71400fea-5c80-4895-a89f-6b19fb325d6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: colorama\n",
            "Successfully installed colorama-0.4.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MDP:\n",
        "    def __init__(self, world, actions, states, terminal_states,\n",
        "                 discount, str_actions, next_states):\n",
        "        self.world = world\n",
        "        self.actions = actions\n",
        "        self.states = states\n",
        "        self.terminal_states = terminal_states\n",
        "        self.discount = discount\n",
        "        self.str_actions = str_actions\n",
        "        self.next_states = next_states"
      ],
      "metadata": {
        "id": "h6-gLLaAMLZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cVzQHKN5RMJ"
      },
      "outputs": [],
      "source": [
        "def print_three_by_three(arr):\n",
        "    for row in range(0, 7, 3):\n",
        "        for col in range(3):\n",
        "            print(arr[row + col], end=' ')\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_converged(curr_v, new_v):\n",
        "    return all(abs(curr_v[s] - new_v[s]) < 1E-8 for s in range(9))"
      ],
      "metadata": {
        "id": "0KMzwF6ASLv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_converged_policy(curr_p, new_p):\n",
        "    return all(curr_p[s] == new_p[s] for s in range(9))"
      ],
      "metadata": {
        "id": "upyxJ5j9Mnp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def str_actions_inverse(policy):\n",
        "  # str_actions = ['u', 'r', 'd', 'l']\n",
        "  if policy == 'u':\n",
        "    return 0\n",
        "  elif policy == 'r':\n",
        "    return 1\n",
        "  elif policy == 'd':\n",
        "    return 2\n",
        "  else:\n",
        "    return 3"
      ],
      "metadata": {
        "id": "wzm4tqIAPYKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sum(curr_state, curr_action, v, mdp):\n",
        "    next_state = mdp.next_states[curr_state][curr_action]\n",
        "    summation = (0.8 * (mdp.world[next_state] + mdp.discount * v[next_state]))\n",
        "\n",
        "    next_state = mdp.next_states[curr_state][(curr_action + 1) % 4]\n",
        "    summation += (0.1 * (mdp.world[next_state] + mdp.discount * v[next_state]))\n",
        "\n",
        "    next_state = mdp.next_states[curr_state][(curr_action - 1) % 4]\n",
        "    summation += (0.1 * (mdp.world[next_state] + mdp.discount * v[next_state]))\n",
        "    return summation"
      ],
      "metadata": {
        "id": "FyYTaTFgd0AA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_max_action(curr_state, old_v, mdp):\n",
        "    if curr_state in mdp.terminal_states:\n",
        "        return 0, 's'\n",
        "    max_val = -1E9\n",
        "    act = 0\n",
        "    for action in mdp.actions:\n",
        "        sigma = get_sum(curr_state, action, old_v, mdp)\n",
        "        if sigma > max_val:\n",
        "            max_val = sigma\n",
        "            act = action\n",
        "    return max_val, mdp.str_actions[act]"
      ],
      "metadata": {
        "id": "JEl-UYGzd46Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MDP_value_iteration(mdp):\n",
        "    old_v = [0 for _ in range(9)]\n",
        "    policy = list(range(9))\n",
        "    while True:\n",
        "        new_value = list(range(9))\n",
        "        for state in mdp.states:\n",
        "            new_value[state], policy[state] = get_max_action(state, old_v, mdp)\n",
        "\n",
        "        if is_converged(old_v, new_value):\n",
        "            old_v = new_value\n",
        "            break\n",
        "        old_v = new_value\n",
        "    print(Fore.GREEN + 'Values:' + Fore.RESET)\n",
        "    old_v = [round(v, 2) for v in old_v]\n",
        "    print_three_by_three(old_v)\n",
        "    print(Fore.GREEN + 'Policy:' + Fore.RESET)\n",
        "    print_three_by_three(policy)\n",
        "    print('\\n------------------------------------------------------------')"
      ],
      "metadata": {
        "id": "riHdP3p2d8P3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MDP_policy_evaluation(mdp, old_v, policy):\n",
        "    new_v = [0 for _ in range(9)]\n",
        "    # print(Fore.GREEN + 'Now evaluating policy:' + Fore.RESET)\n",
        "    # print_three_by_three(policy)\n",
        "    while True:\n",
        "        for state in mdp.states:\n",
        "            if state in mdp.terminal_states:\n",
        "              new_v[state] = 0 # mdp.world[state]\n",
        "              continue\n",
        "            action = str_actions_inverse(policy[state])\n",
        "            new_v[state] = get_sum(state, action, old_v, mdp)\n",
        "\n",
        "        if is_converged(old_v, new_v):\n",
        "            old_v = new_v\n",
        "            break\n",
        "        old_v = new_v\n",
        "    # print(Fore.GREEN + 'Values:' + Fore.RESET)\n",
        "    old_v = [round(v, 2) for v in old_v]\n",
        "    # print_three_by_three(old_v)\n",
        "    # print('\\n------------------------------------------------------------')\n",
        "    return old_v"
      ],
      "metadata": {
        "id": "R7s5atK88pm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MDP_policy_extraction(mdp, v):\n",
        "    policy = list(range(9))\n",
        "    for state in mdp.states:\n",
        "        if state in mdp.terminal_states:\n",
        "          policy[state] = 's'\n",
        "          continue\n",
        "        max_val = -1E9\n",
        "        max_action = 0\n",
        "        for action in mdp.actions:\n",
        "            val = get_sum(state, action, v, mdp)\n",
        "            if val > max_val:\n",
        "                max_val = val\n",
        "                max_action = action\n",
        "        policy[state] = mdp.str_actions[max_action]\n",
        "    # print(Fore.GREEN + 'Policy extraction successful:' + Fore.RESET)\n",
        "    # print_three_by_three(policy)\n",
        "    # print('\\n------------------------------------------------------------')\n",
        "    return policy"
      ],
      "metadata": {
        "id": "FmzzG9o5-wiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MDP_policy_iteration(mdp):\n",
        "    policy = ['u' for _ in range(9)]\n",
        "    new_value = [0 for _ in range(9)]\n",
        "    while True:\n",
        "        new_value = MDP_policy_evaluation(mdp, new_value, policy)\n",
        "        new_policy = MDP_policy_extraction(mdp, new_value)\n",
        "        if is_converged_policy(new_policy, policy):\n",
        "            break\n",
        "        policy = new_policy\n",
        "    print(Fore.GREEN + 'Final values:' + Fore.RESET)\n",
        "    print_three_by_three(new_value)\n",
        "    print(Fore.GREEN + 'Final policy:' + Fore.RESET)\n",
        "    print_three_by_three(policy)\n",
        "    print('\\n------------------------------------------------------------')"
      ],
      "metadata": {
        "id": "Mbu4RFqg__9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    world = [-1 for _ in range(9)]\n",
        "    world[2] = 10\n",
        "\n",
        "    rs = [100, 3, 0, -3]\n",
        "    actions = [0, 1, 2, 3]\n",
        "    states = range(9)\n",
        "    terminal_states = (0, 2)\n",
        "    discount = 0.99\n",
        "    str_actions = ['u', 'r', 'd', 'l']\n",
        "\n",
        "    next_states = [\n",
        "        [0, 1, 3, 0],\n",
        "        [1, 2, 4, 0],\n",
        "        [2, 2, 5, 1],\n",
        "        [0, 4, 6, 3],\n",
        "        [1, 5, 7, 3],\n",
        "        [2, 5, 8, 4],\n",
        "        [3, 7, 6, 6],\n",
        "        [4, 8, 7, 6],\n",
        "        [5, 8, 8, 7]\n",
        "    ]\n",
        "\n",
        "    mdp = MDP(world, actions, states, terminal_states,\n",
        "              discount, str_actions, next_states)\n",
        "\n",
        "    print(Fore.GREEN + 'Running value iteration:' + Fore.RESET)\n",
        "    for r in rs:\n",
        "        world[0] = r\n",
        "        print('\\nr -> ' + str(r))\n",
        "        MDP_value_iteration(mdp)\n",
        "\n",
        "\n",
        "    print(Fore.GREEN + 'Running policy iteration:' + Fore.RESET)\n",
        "    for r in rs:\n",
        "        world[0] = r\n",
        "        print('\\nr -> ' + str(r))\n",
        "        MDP_policy_iteration(mdp)\n"
      ],
      "metadata": {
        "id": "1eHCG-RHIA4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7f5QcIWIHMV",
        "outputId": "6152f94c-b2a5-4e5e-c3df-c6874ec32a04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mRunning value iteration:\u001b[39m\n",
            "\n",
            "r -> 100\n",
            "\u001b[32mValues:\u001b[39m\n",
            "0 99.2 0 \n",
            "99.2 96.72 90.11 \n",
            "96.45 94.3 91.68 \n",
            "\u001b[32mPolicy:\u001b[39m\n",
            "s l s \n",
            "u l d \n",
            "u l l \n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "r -> 3\n",
            "\u001b[32mValues:\u001b[39m\n",
            "0 9.56 0 \n",
            "6.45 8.2 9.56 \n",
            "5.63 6.86 8.05 \n",
            "\u001b[32mPolicy:\u001b[39m\n",
            "s r s \n",
            "r r u \n",
            "r r u \n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "r -> 0\n",
            "\u001b[32mValues:\u001b[39m\n",
            "0 9.56 0 \n",
            "6.14 8.2 9.56 \n",
            "5.6 6.86 8.05 \n",
            "\u001b[32mPolicy:\u001b[39m\n",
            "s r s \n",
            "r r u \n",
            "r r u \n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "r -> -3\n",
            "\u001b[32mValues:\u001b[39m\n",
            "0 9.56 0 \n",
            "5.84 8.2 9.56 \n",
            "5.56 6.86 8.05 \n",
            "\u001b[32mPolicy:\u001b[39m\n",
            "s r s \n",
            "r r u \n",
            "r r u \n",
            "\n",
            "------------------------------------------------------------\n",
            "\u001b[32mRunning policy iteration:\u001b[39m\n",
            "\n",
            "r -> 100\n",
            "\u001b[32mFinal values:\u001b[39m\n",
            "0 99.18 0 \n",
            "99.18 96.67 89.35 \n",
            "96.39 94.21 91.46 \n",
            "\u001b[32mFinal policy:\u001b[39m\n",
            "s l s \n",
            "u l d \n",
            "u l l \n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "r -> 3\n",
            "\u001b[32mFinal values:\u001b[39m\n",
            "0 9.51 0 \n",
            "5.95 8.03 9.53 \n",
            "4.85 6.49 7.95 \n",
            "\u001b[32mFinal policy:\u001b[39m\n",
            "s r s \n",
            "r r u \n",
            "r r u \n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "r -> 0\n",
            "\u001b[32mFinal values:\u001b[39m\n",
            "0 9.5 0 \n",
            "5.57 8.02 9.53 \n",
            "4.71 6.46 7.94 \n",
            "\u001b[32mFinal policy:\u001b[39m\n",
            "s r s \n",
            "r r u \n",
            "r r u \n",
            "\n",
            "------------------------------------------------------------\n",
            "\n",
            "r -> -3\n",
            "\u001b[32mFinal values:\u001b[39m\n",
            "0 9.11 0 \n",
            "2.86 7.0 9.34 \n",
            "1.38 4.81 7.48 \n",
            "\u001b[32mFinal policy:\u001b[39m\n",
            "s r s \n",
            "r r u \n",
            "r r u \n",
            "\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For r = 100:   \n",
        "**Values**:    \n",
        "0 99.2 0    \n",
        "99.2 96.72 90.11    \n",
        "96.45 94.3 91.68    \n",
        "**Policy**:   \n",
        "s  l  s    \n",
        "u  l  d    \n",
        "u  l  l   \n",
        "\n",
        "We notice that the optimal policy for the cell (1, 2) is to go down. This is done to avoid the probability of going to the cell that has a +10. This way, we'll have a higher chance of going to the cell that has +100.     \n",
        "We also notice that the value of the rewards gets bigger and bigger once we get close to the top left cell (+100)\n"
      ],
      "metadata": {
        "id": "mvmm_2enJFO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "r -> 3    \n",
        "**Values**:    \n",
        "0 9.56 0     \n",
        "6.45 8.2 9.56     \n",
        "5.63 6.86 8.05     \n",
        "**Policy**:   \n",
        "s r s     \n",
        "r r u     \n",
        "r r u     \n",
        "\n",
        "We notice that the value of r is not good enough for the algorithm to gain, gaining a +10 is much better in this case. So the optimal policy tries to avoid going to r and instead, tries to go to the +10 square. This idea also applies to values of 0, -3. We get the same optimal policy, which is always try to go to the +10 square.\n",
        "\n",
        "We also notice that the cell (1,0) is r. This means that there is a probabilty that we hit the top left cell early. This is ok in this case as going down will lower our score by alot, which is not worth it at all in any case.\n"
      ],
      "metadata": {
        "id": "whk9lwwtJ6EJ"
      }
    }
  ]
}